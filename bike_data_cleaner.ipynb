{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e6e79a-0b69-4d87-999f-d27cf12e3db9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# <span style='background:yellow'> Pacer's BikeShare Data Cleanup and Visualization </span>\n",
    "##### The purpose of the following code is to visualize and clean bikeshare data placed in the `kiosk_data` folder. The graphs and visualizations generated are placed in the '`month_histogram`' and '`other_graphs`' folders. There is a `gis_compatible.csv` file generated by this program to more-easily visualize bike station usage within GIS software.\n",
    "   ---\n",
    "*The code that cleans the data is hidden by default**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113235a-0108-45f3-9cd7-842a04b45847",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 1: Import Needed Libraries & Files\n",
    "---\n",
    "First, we will import the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e71eb-52ef-4168-906a-b31a0b3b3f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # for manipulating .csv files as dataframes\n",
    "import glob #for importing files\n",
    "import os # for navigating folders\n",
    "import matplotlib.pyplot as plt # for graphs\n",
    "import seaborn as sns # another graphing library\n",
    "import calendar\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd326f-66df-4fab-8d32-5a3ebf03018a",
   "metadata": {},
   "source": [
    "Next, we will import the needed files.\n",
    "- After the needed libraries have been imported, the data needs to be loaded into python. This will be done by loading all the .csv files into a dataframe. A problem that arises in doing this is that there are potentially many files being imported. We don't know the names of these files, or how many there will be. To remedy this problem, we will use both the `glob` and `os` libraries to:\n",
    "> 1) Get the present working directory (folder) with `os.getcwd` and save this path to `path`.\n",
    "> 2) Save the names of all the .csv files in the kiosk_data folder as a list of strings `all_files`. This is accomplished through the `glob` library\n",
    "> 3) Iterate through every file in the newly created list, save each file as a dataframe, and then merge all the dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c09d9a-0a75-4907-b73d-b67ddf49b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd() gets the present working directory (pwd) AKA *current folder*\n",
    "all_files = glob.glob(os.path.join(os.getcwd(), \"kiosk_data\", \"*.csv\"))\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0,encoding='latin-1')\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df873c01-c452-4a48-bf05-5824e49e37e4",
   "metadata": {},
   "source": [
    "The station locations must be loaded too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9998b88-58c2-463b-b6cd-bfe62845cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations  = pd.read_csv(os.path.join(os.getcwd(), 'station_location.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bfc66-e0eb-41ff-b96c-87fb272bf3ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 2: Understand the Data\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d4cde-9bdc-4abf-a2f9-1dfbe6ed1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edf2945-17b7-41d6-9864-93c41c4bab06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 3: Clean the Data\n",
    "---\n",
    "- As seen in the output above, a lot of the data generated is from `\"Maintenance\"`, this presumably represents maintenance workers, not end users. Let's see how many of these entries there are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8929618-2408-44d7-bb9a-e01bcdc404ef",
   "metadata": {},
   "source": [
    "How many maintenance rows are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830f108-5e99-45d0-b879-a14fd8bf1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.UserRole.str.contains(\"Maintenance\") == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242e5ae-f1f6-4786-b81e-9f09b461f6f6",
   "metadata": {},
   "source": [
    "### Removing maintenance rows\n",
    "- The dataframe without maintenance rows will be saved as `pacer_data`. The following snippet shows how much this reduction saved in terms of dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7692b3-f51d-4af4-904b-6ff71a7adf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing maintenance roles.\n",
    "pacer_data = df[df.UserRole.str.contains(\"Maintenance\") == False]\n",
    "print(f'Size before maintenance deletion: {df.size}\\n Size after maintenance deletion: {pacer_data.size}')\n",
    "print(f'Length before maintenance deletion: {len(df)}\\n Length after maintenance deletion: {len(pacer_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320479ce-0873-4521-859f-81545d72f9f1",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns from dataframe\n",
    "- There were a lot of columns in the dataframe that won't be used. They'll be omitted to allow for faster runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb60360-fcc7-481e-b77a-d83ead6fe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping most columns from main dataframe...\n",
    "old_size = pacer_data.size\n",
    "pacer_data = pacer_data.drop(['TripId','UserProgramName','UserCity','UserState','UserZip','UserCountry','MembershipType','Bike','BikeType',\n",
    "                 'UserId', 'UserRole','DurationMins','AdjustedDurationMins','UsageFee','AdjustmentFlag',\n",
    "                  'EstimatedCaloriesBurned','LocalProgramFlag','TripRouteCategory','TripProgramName'], axis=1)\n",
    "print(f'Size before column deletion: {old_size}\\n Size after column deletion: {pacer_data.size}')\n",
    "print(f'Net decrease in size: {(pacer_data.size / old_size):2.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff406d82-628b-43c0-a0f2-748403002801",
   "metadata": {},
   "source": [
    "### Sorting by checkout date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a841ac4-602b-40a7-90e2-9db16b71559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_data = pacer_data.sort_values(by=\"CheckoutDateLocal\")\n",
    "pacer_data = pacer_data.reset_index(drop=True)\n",
    "pacer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b131cb9-1874-49cd-8575-0ba9d70f40ff",
   "metadata": {},
   "source": [
    "This finds the start and end dates of the table\n",
    " (This will be useful in naming file output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b33850-a524-44cb-a602-0927733b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pacer_data.at[pacer_data.index[0], \"CheckoutDateLocal\"]\n",
    "end_date = pacer_data.at[pacer_data.index[-1], \"CheckoutDateLocal\"]\n",
    "print(f'The table represents trips spanning from {start_date} to {end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a7c8b-af3b-4554-a79f-6937bc518d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the start and end dates as datetime, taking the difference, then taking the absolute value.\n",
    "total_days = abs((datetime.strptime(start_date, '%Y-%m-%d') - datetime.strptime(end_date, '%Y-%m-%d')).days)\n",
    "total_days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c079c2-0934-45a4-bcea-ca29ee338572",
   "metadata": {},
   "source": [
    "Adding additional columns for latitude and longitude and populating them with a default value of zero. This creates \"empty\" columns that will then be filled by adding the coordinates of their respective kiosks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc794780-6d3e-4db6-930a-5b9fbd6754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d.insert(\"column index\", \"column name\", \"default value\") \n",
    "pacer_data.insert(1, \"Checkout_Latitude\", 0)\n",
    "pacer_data.insert(2, \"Checkout_Longitude\", 0)\n",
    "pacer_data.insert(4, \"Checkin_Latitude\", 0)\n",
    "pacer_data.insert(5, \"Checkin_Longitude\", 0)\n",
    "pacer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b608f81-9778-4a7c-bdcb-4e90fb1b9d27",
   "metadata": {},
   "source": [
    "- The DATAFRAME and CSV populate differently. An example of this is the `Michigan St. and N White River Pkwy.` location (it has an an additional space after the final period). The following code attempts to populate the longitude and latitude columns for these \"bad\" locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0c894-e967-4545-88f0-3f438f750c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_data['CheckoutKioskName'].replace({\"Michigan St. and N White River Pkwy. \": \"Michigan St. and N White River Pkwy.\",\n",
    "                                         \"Michigan and Blackford\":\"Michigan St. and Blackford\",\n",
    "                                        \"Michigan and Senate\":\"Michigan St. and Senate\",\n",
    "                                        \"State Fairgrounds and Monon Trail\":\"State Fairgrounds at 38th St. and Monon Trail\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680e885-62df-43ed-88f0-cace112f160d",
   "metadata": {},
   "source": [
    "- This code loops through all the station names in the `station_locations.csv` folder and gets the latitude and longitude from them to add to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca2e12-a8aa-4b9c-a0f0-26b16251d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(locations['Station Name'])):\n",
    "    #checkout kiosk\n",
    "    pacer_data.loc[pacer_data['CheckoutKioskName'] == locations['Station Name'][i], 'Checkout_Latitude'] = locations.Latitude[i]\n",
    "    pacer_data.loc[pacer_data['CheckoutKioskName'] == locations['Station Name'][i], 'Checkout_Longitude'] = locations.Longitude[i]\n",
    "    #return kiosk\n",
    "    pacer_data.loc[pacer_data['ReturnKioskName'] == locations['Station Name'][i], 'Checkin_Latitude'] = locations.Latitude[i]\n",
    "    pacer_data.loc[pacer_data['ReturnKioskName'] == locations['Station Name'][i], 'Checkin_Longitude'] = locations.Longitude[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279fee0-9a32-41d2-9bce-c11710ff2b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934377c-dcb5-499f-b3d0-8e2cae4ec139",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_locations = pacer_data.loc[pacer_data['Checkout_Latitude'] == 0, 'CheckoutKioskName'].unique()\n",
    "for location in bad_locations:\n",
    "    print(location)\n",
    "bad_locations\n",
    "# Washington and Illinois wasn't listed as a station anyways. Disregard.\n",
    "# Headquarters is all maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6423e-8b8e-4a99-b57d-7d9c0a25f7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_data = pacer_data[pacer_data['Checkout_Latitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd324b-1dc7-46e7-8ee3-c9961298cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pacer_data.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaff2a9-90e4-4a95-9663-2be288fc6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_offset = pacer_data['EstimatedCarbonOffset'].sum()\n",
    "print(f'The total carbon offset is {carbon_offset/1000:,.2f} kg CO2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154b7de-4416-46e8-8bb7-91e78fa20d27",
   "metadata": {},
   "source": [
    "### Populating New Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb6dbe-d1ab-4311-a99c-9fc8c131fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(len(pacer_data[\"CheckoutDateLocal\"])) + ' ' + str(len(pacer_data[\"CheckoutTimeLocal\"])))\n",
    "pacer_data[\"CheckoutDateTime\"] = pacer_data['CheckoutDateLocal'] + ' ' + pacer_data['CheckoutTimeLocal']\n",
    "pacer_data[\"ReturnDateTime\"] = pacer_data['ReturnDateLocal'] + ' ' + pacer_data['ReturnTimeLocal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3b257-818b-48e9-ac82-50fbd216254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_data[\"CheckoutDateTime\"] = pd.to_datetime(pacer_data[\"CheckoutDateTime\"])\n",
    "pacer_data[\"ReturnDateTime\"] = pd.to_datetime(pacer_data[\"ReturnDateTime\"])\n",
    "pacer_data[\"CheckoutDateLocal\"] = pd.to_datetime(pacer_data[\"CheckoutDateLocal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc22034-bc24-4d90-9e30-aeb69ffb36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see when the weekend checkouts are\n",
    "pacer_data[\"IsWeekend\"] = pacer_data[\"CheckoutDateLocal\"].dt.weekday >= 5\n",
    "print(\"Total Checkouts: \" + str(len(pacer_data[pacer_data[\"IsWeekend\"] == True])))\n",
    "print(\"Weekend Checkouts: \" + str(len(pacer_data[pacer_data[\"IsWeekend\"] == True])))\n",
    "print(\"Weekday Checkouts: \" + str(len(pacer_data[pacer_data[\"IsWeekend\"] == False])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdad51-a317-4fa8-b0ad-959b24a66d97",
   "metadata": {},
   "source": [
    "# Step 4: Visualize the Data\n",
    "---\n",
    "## Histograms: Month and Year\n",
    "- This program was made to display different times separately, namely: **weekends vs weekdays** as well as **months**, and **years**\n",
    "    - The different time categories will be distinguished before setting them in a `for` loop...\n",
    "        - A function will iterate through every *unique* year, followed by every *unique* month in that year.\n",
    "        - This will create a histogram for each year (saved in `other_graphs`) and a histogram for each month (saved in `monthly_histograms`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab5668-c5ba-4501-ab8a-c6ccbcbdb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through years\n",
    "for year in pacer_data['CheckoutDateTime'].dt.year.unique():\n",
    "    print('---\\n', year ,'\\n---')\n",
    "    year_pacer_data = pacer_data[pacer_data['CheckoutDateTime'].dt.year == year]\n",
    "    \n",
    "    numeric_time_series = year_pacer_data[\"CheckoutDateTime\"].dt.hour + .01*year_pacer_data[\"CheckoutDateTime\"].dt.minute\n",
    "    year_hist = sns.histplot(data=numeric_time_series, bins=24, element=\"step\", kde=True)\n",
    "    plt.xlabel(\"Time of Day\", fontsize=12)\n",
    "    plt.title(f\"Checkouts by Time of Day: {year}\", fontsize=12)\n",
    "    year_hist2 = year_hist.get_figure()\n",
    "    year_hist2.savefig(str(os.path.join(os.getcwd(),'other_graphs', f'{year}_checkout_summary')))\n",
    "    plt.clf() # This clears the figure so it doesn't overlap.\n",
    "    print(str(os.path.join(os.getcwd(),'other_graphs', f'{year}_checkout_summary')))\n",
    "    \n",
    "    # now iterate across the months\n",
    "    for month in year_pacer_data['CheckoutDateTime'].dt.month.unique():\n",
    "        ##\n",
    "        month_word = calendar.month_name[month]\n",
    "        #create a month dataframe out of the year dataframe...\n",
    "        month_pacer_data = year_pacer_data[year_pacer_data['CheckoutDateTime'].dt.month == month]\n",
    "        month_pacer_data = pd.to_datetime(month_pacer_data['CheckoutDateTime'])\n",
    "        numeric_time_series = month_pacer_data.dt.hour + .01*month_pacer_data.dt.minute\n",
    "        ##\n",
    "        #------- P L O T T I N G -------\n",
    "        ##\n",
    "        sns_hist = sns.histplot(data=numeric_time_series, bins=24, element=\"step\", kde=True)\n",
    "        plt.xlabel(\"Time of Day\", fontsize=12)\n",
    "        plt.title(f\"Checkouts by Time of Day: {month_word} {year}\", fontsize=12)\n",
    "        #plt.set_xticks([], minor=True)\n",
    "        shist = sns_hist.get_figure()\n",
    "        month_file = f'checkout_histogram_{month}_{year}'\n",
    "        shist.savefig(str(os.path.join(os.getcwd(), 'month_histograms', month_file)))\n",
    "        plt.clf() # This clears the figure so it doesn't overlap.\n",
    "        print(os.path.join(os.getcwd(), 'month_histograms', month_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562a348-739f-4ca3-9cd9-03f7e439fa13",
   "metadata": {},
   "source": [
    "## Creating the Complete Histogram\n",
    "- This histogram spans the entire duration of the months provided in `kiosk_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658bbe1-5b59-4e55-b701-d83eda906da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_time_series = pacer_data[\"CheckoutDateTime\"].dt.hour + .01*pacer_data[\"CheckoutDateTime\"].dt.minute\n",
    "sns_hist = sns.histplot(data=numeric_time_series, bins=24, element=\"step\", kde=True)\n",
    "plt.xlabel(\"Time of Day\", fontsize=12)\n",
    "plt.title(f\"Checkouts by Time of Day: {start_date} to {end_date}\", fontsize=12)\n",
    "#plt.set_xticks([], minor=True)\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs', f'complete_checkout_histogram_{start_date}_{end_date}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c84b4-d769-44d7-a7a7-3877a5ab24d0",
   "metadata": {},
   "source": [
    "***\n",
    "## Creating a Formatted `.csv` output file\n",
    "- Getting Total Checkout counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6c6bc-2432-45e7-b65f-d5dfbe226379",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d62978-9f9c-4076-a5e8-59e6ef4ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_check_return(pacer_data, summary_df):\n",
    "    summary_df.insert(len(summary_df.columns), \"Checkout Count\", 0)\n",
    "    #populate checkout counts\n",
    "    checkout_counts =pacer_data['CheckoutKioskName'].value_counts()\n",
    "    checkcount1 = checkout_counts.to_dict() #converts to dictionary\n",
    "    summary_df['Checkout Count'] = summary_df['Station Name'].map(checkcount1) \n",
    "count_check_return(pacer_data, summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a2fa0-eaf4-4a70-8df4-832d6d7f6631",
   "metadata": {},
   "source": [
    "- This creates a csv file containing:\n",
    "    - All the station names\n",
    "    - The GPS coordinates for each station name (World Mercator 84)\n",
    "    - The **Total Checkouts** for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016a2e4-5911-4d95-af1c-ff6e7c8291d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.to_csv('gis_compatible.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b360e35-a7c4-4a43-a755-99bb8a6401e9",
   "metadata": {},
   "source": [
    "## Study: Visualizing the total Carbon Offset over Time\n",
    "- A good way to visualize this is through a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d7f1e-73b6-4b54-b175-dc33010d478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = []\n",
    "annual_carbon_offset = []\n",
    "net_carbon_offset = []\n",
    "\n",
    "for year in pacer_data['CheckoutDateTime'].dt.year.unique():\n",
    "    year_pacer_data = pacer_data[pacer_data['CheckoutDateTime'].dt.year == year]\n",
    "    #year\n",
    "    years.append(year)\n",
    "    #annual carbon offet\n",
    "    carbon_offset = year_pacer_data['EstimatedCarbonOffset'].sum() / 1000\n",
    "    annual_carbon_offset.append(carbon_offset)\n",
    "    # net carbon offset\n",
    "    net_carbon_offset.append(sum(annual_carbon_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97e124d-4d45-44de-903e-23a557354c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_offset_df = pd.DataFrame([years, annual_carbon_offset, net_carbon_offset]).transpose()\n",
    "carbon_offset_df.columns = ['Year', 'Annual CO2 Offset', 'Net CO2 Offset']\n",
    "carbon_offset_df['Year'] = carbon_offset_df['Year'].astype(int)\n",
    "carbon_offset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e09bf-cb1a-484b-b953-681e9213b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "tidy = carbon_offset_df.melt(id_vars='Year').rename(columns=str.title)\n",
    "sns.barplot(x='Year', y='Value', hue='Variable', palette=\"ch:s=2.2,rot=0,dark=0.4, light=0.8\", data=tidy, ax=ax1)\n",
    "plt.ylabel(\"Carbon Offset (kg)\", fontsize=12)\n",
    "sns.despine(fig)\n",
    "plt.gcf().subplots_adjust(left=0.2)\n",
    "plt.legend()\n",
    "plt.title('Carbon Reduction by Year')\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs','annual_carbon_reduction')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964cf21c-d7f1-412e-8c82-e29b2c2eba02",
   "metadata": {},
   "source": [
    "## Study: Weekends vs Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604b2bd6-172b-495d-9bc0-95d963637ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pacer_weekend = pacer_data[pacer_data[\"IsWeekend\"]==True]\n",
    "pacer_weekday = pacer_data[pacer_data[\"IsWeekend\"]==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6a100-9d38-4888-925b-28ae6a8fcbe9",
   "metadata": {},
   "source": [
    "####  Average Checkouts Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a205a-09b8-4058-8fd5-d7480b837068",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_checkouts_weekend = len(pacer_weekend) / (2 * total_days)\n",
    "average_checkouts_weekday = len(pacer_weekday) / (5 * total_days)\n",
    "\n",
    "print(f'average_checkouts_weekend: {average_checkouts_weekend:.2f} \\naverage_checkouts_weekday: {average_checkouts_weekday:.2f}')\n",
    "\n",
    "sns.barplot(\n",
    "    x=['Weekend', 'Weekday'], \n",
    "    y=[average_checkouts_weekend, average_checkouts_weekday], \n",
    "    estimator=sum, \n",
    "    ci=None,\n",
    "    palette='muted');\n",
    "plt.title('Average Checkouts Per Day')\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs','checkouts_weekends_v_weekdays')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e800f0-27cb-440c-be30-551aa84a9e08",
   "metadata": {},
   "source": [
    "### Commutes over 30 Minutes - Weekends vs. Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad062c-a507-4894-9b5a-e5dc0cac7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_30minplus_weekend = 100 *len(pacer_weekend[pacer_weekend['TripOver30Mins'] == 'Y']) / len(pacer_weekend)\n",
    "percent_30minplus_weekday = 100 *len(pacer_weekday[pacer_weekday['TripOver30Mins'] == 'Y']) / len(pacer_weekday)\n",
    "\n",
    "sns.barplot(\n",
    "    x=['Weekend', 'Weekday'], \n",
    "    y=[percent_30minplus_weekend, percent_30minplus_weekday], \n",
    "    estimator=sum, \n",
    "    ci=None,\n",
    "    palette='muted');\n",
    "plt.title('Percent of Checkouts over 30 Minutes')\n",
    "\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs','over_30min_checkouts_weekends_days')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77ceeb-14b7-46df-a040-1b9ec186f6ae",
   "metadata": {},
   "source": [
    "### Usage Patterns - Weekends vs. Weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc9f72-f9a4-4fc9-95dc-7ad0bdaa17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_time_series_wkend = pacer_weekend[\"CheckoutDateTime\"].dt.hour + .01*pacer_weekend[\"CheckoutDateTime\"].dt.minute\n",
    "numeric_time_series_wkday = pacer_weekday[\"CheckoutDateTime\"].dt.hour + .01*pacer_weekday[\"CheckoutDateTime\"].dt.minute\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,5))\n",
    "# fig.suptitle('1 row x 2 columns axes with no data')\n",
    "\n",
    "# Weekend\n",
    "axes[0].set_title('Weekend Checkouts')\n",
    "weekend_histplot = sns.histplot(data=numeric_time_series_wkend, bins=24, element=\"step\", kde=True, ax=axes[0])\n",
    "weekend_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "\n",
    "# Weekday\n",
    "axes[1].set_title('Weekday Checkouts')\n",
    "weekday_histplot = sns.histplot(data=numeric_time_series_wkday, bins=24, element=\"step\", kde=True, ax=axes[1])\n",
    "weekday_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs',f'weekend_checkout_histogram_{start_date}_{end_date}')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf902c-8dd8-443e-8203-c33313c7bf93",
   "metadata": {},
   "source": [
    "## Study: Top 5 Busiest Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07005a9a-779f-44ad-bfa6-669fdcf59a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = summary_df.sort_values(by=\"Checkout Count\", ascending=False)[0:5].iloc[:, [0, 3]]\n",
    "top_5['Percent of Total'] = top_5[\"Checkout Count\"]/ summary_df[\"Checkout Count\"].sum() * 100\n",
    "top_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615fac2-19d3-45bc-ac9d-6a7fa27ba0d8",
   "metadata": {},
   "source": [
    "## Study: 5 Least Busy Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2823f0-d8dd-4db7-8406-4b5e8b189c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_5 = summary_df.sort_values(by=\"Checkout Count\", ascending=True)[0:5].iloc[:, [0, 3]]\n",
    "bottom_5['Percent of Total'] = bottom_5[\"Checkout Count\"]/ summary_df[\"Checkout Count\"].sum() * 100\n",
    "bottom_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03731226-5071-46cc-acbc-987d364d8d2e",
   "metadata": {},
   "source": [
    "## Study: Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad2ea4-9675-446b-afed-7fd4678e2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The seasons are saved as numbers...\n",
    "pacer_data['CheckoutDateTime'].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09edc79-4d23-4e15-a67b-491ea227b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the data out into smaller 'season' tables\n",
    "winter_pacer_data = pd.concat(\n",
    "    [pacer_data[pacer_data['CheckoutDateTime'].dt.month == 12],\n",
    "     pacer_data[pacer_data['CheckoutDateTime'].dt.month.between(1, 2)]])\n",
    "spring_pacer_data = pacer_data[pacer_data['CheckoutDateTime'].dt.month.between(3, 5)]\n",
    "summer_pacer_data = pacer_data[pacer_data['CheckoutDateTime'].dt.month.between(6, 8)]\n",
    "fall_pacer_data = pacer_data[pacer_data['CheckoutDateTime'].dt.month.between(9, 11)]\n",
    "# these time series are a condensed way to visualize the time\n",
    "numeric_time_series_winter = winter_pacer_data[\"CheckoutDateTime\"].dt.hour + .01*winter_pacer_data[\"CheckoutDateTime\"].dt.minute\n",
    "numeric_time_series_spring = spring_pacer_data[\"CheckoutDateTime\"].dt.hour + .01*spring_pacer_data[\"CheckoutDateTime\"].dt.minute\n",
    "numeric_time_series_summer = summer_pacer_data[\"CheckoutDateTime\"].dt.hour + .01*summer_pacer_data[\"CheckoutDateTime\"].dt.minute\n",
    "numeric_time_series_fall = fall_pacer_data[\"CheckoutDateTime\"].dt.hour + .01*fall_pacer_data[\"CheckoutDateTime\"].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99685927-0e0f-4376-8f18-817f2ee3e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16,14))\n",
    "# Winter\n",
    "axes[0, 0].set_title('Winter', fontsize = 18)\n",
    "winter_histplot = sns.histplot(data=numeric_time_series_winter, bins=24, element=\"step\", kde=True, ax=axes[0,0])\n",
    "winter_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "# Spring\n",
    "axes[0, 1].set_title('Spring', fontsize = 18)\n",
    "spring_histplot = sns.histplot(data=numeric_time_series_spring, bins=24, element=\"step\", color='green', kde=True, ax=axes[0,1])\n",
    "spring_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "# Fall\n",
    "axes[1, 1].set_title('Fall', fontsize = 18)\n",
    "fall_histplot = sns.histplot(data=numeric_time_series_fall, bins=24, element=\"step\", kde=True, color='orange', ax=axes[1,1])\n",
    "fall_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "# Summer\n",
    "axes[1, 0].set_title('Summer', fontsize = 18)\n",
    "summer_histplot = sns.histplot(data=numeric_time_series_summer, bins=24, element=\"step\", kde=True, color='r', ax=axes[1,0])\n",
    "summer_histplot.set_xlabel(\"Time of Day\", fontsize = 12)\n",
    "\n",
    "plt.savefig(str(os.path.join(os.getcwd(),'other_graphs','seasonal_histograms')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
